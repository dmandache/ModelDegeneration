{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FLAG = True\n",
    "DATA_SET = 'Shapes'\n",
    "#DATA_SET = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_grid(data, grid_size=4):\n",
    "    mat_data = data.data if hasattr(data, 'data') else data\n",
    "\n",
    "    _, ax = plt.subplots(nrows=grid_size, ncols=grid_size, figsize=(8,8))\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            ax[i][j].imshow(mat_data[i*grid_size+j].movedim(0, 2), cmap='gray')\n",
    "            ax[i][j].axis('off')\n",
    "    plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils.data import sample_indices\n",
    "\n",
    "\n",
    "def load_shapes_dataset(dirpath='/mnt/Shared/shapes'):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    label_dict = {\n",
    "        'circles' : 0,\n",
    "        'squares' : 1,\n",
    "        'triangles' : 2\n",
    "    }\n",
    "\n",
    "    files = glob.glob(dirpath + '/**/*.png', recursive=True)\n",
    "\n",
    "    for f in files:\n",
    "        img=Image.open(f)\n",
    "        img=img.resize(size=(28,28))\n",
    "        img=img.convert('L')\n",
    "        x.append(np.array(img))\n",
    "        label = f.split('/')[-2]\n",
    "        #y.append(label_dict[label])\n",
    "        y.append(label)\n",
    "        del img\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 28, 28)\n",
      "torch.Size([0, 1, 28, 28]) torch.Size([0, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m mnist_trainset\u001b[38;5;241m.\u001b[39mdata[test_indeces]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mshape, eval_dataset\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, train_dataset\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     28\u001b[0m plot_grid(train_dataset)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "source": [
    "if DATA_SET == 'Shapes' :\n",
    "    shapes, targets = load_shapes_dataset()\n",
    "    shapes = 1 - shapes.reshape(-1, 1, 28, 28) / 255.\n",
    "    print(shapes.shape)\n",
    "\n",
    "\n",
    "    train_indeces = sample_indices(targets, k=90, seed=42)\n",
    "    remaining_indeces = list(set(range(len(targets)))-set(train_indeces))\n",
    "    test_indeces = sample_indices(targets[remaining_indeces], k=10, seed=42)\n",
    "        \n",
    "    train_dataset = torch.from_numpy(shapes[train_indeces].astype(np.float32))\n",
    "    eval_dataset = torch.from_numpy(shapes[test_indeces].astype(np.float32))\n",
    "\n",
    "\n",
    "elif DATA_SET == 'MNIST' :\n",
    "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "    train_indeces = sample_indices(mnist_trainset.targets, k=50, seed=42)\n",
    "    remaining_indeces = list(set(range(len(mnist_trainset.targets)))-set(train_indeces))\n",
    "    test_indeces = sample_indices(mnist_trainset.targets[remaining_indeces], k=5, seed=42)\n",
    "\n",
    "    train_dataset = mnist_trainset.data[train_indeces].reshape(-1, 1, 28, 28) / 255.\n",
    "    eval_dataset = mnist_trainset.data[test_indeces].reshape(-1, 1, 28, 28) / 255.\n",
    "\n",
    "print(train_dataset.shape, eval_dataset.shape)\n",
    "print(train_dataset.min(), train_dataset.max())\n",
    "\n",
    "plot_grid(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean, train_std = train_dataset.mean(), train_dataset.std()\n",
    "# train_dataset -= train_mean / train_std\n",
    "# eval_dataset -= train_mean / train_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythae.models import *\n",
    "from pythae.models.nn.benchmarks.mnist import *\n",
    "from pythae.models.nn.default_architectures import *\n",
    "from pythae.models.base.base_utils import ModelOutput\n",
    "from utils.models import Encoder_VAE_TinyMLP, Decoder_AE_TinyMLP\n",
    "\n",
    "architecture_dict = {\n",
    "    'tiny':\n",
    "        {\n",
    "        'encoder': Encoder_VAE_TinyMLP,\n",
    "        'decoder': Decoder_AE_TinyMLP,\n",
    "        },\n",
    "    'mlp':\n",
    "        {\n",
    "        'encoder': Encoder_VAE_MLP,\n",
    "        'decoder': Decoder_AE_MLP,\n",
    "        },\n",
    "    'convnet':\n",
    "        {\n",
    "        'encoder': Encoder_Conv_VAE_MNIST,\n",
    "        'decoder': Decoder_Conv_AE_MNIST,\n",
    "        },\n",
    "    'resnet':\n",
    "        {\n",
    "        'encoder': Encoder_ResNet_VAE_MNIST,\n",
    "        'decoder': Decoder_ResNet_AE_MNIST,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# model_config = RHVAEConfig(\n",
    "#     input_dim=(1, 28, 28),\n",
    "#     latent_dim=8,\n",
    "#     reconstruction_loss=\"mse\",\n",
    "#     n_lf=3,\n",
    "#     eps_lf=1e-3,\n",
    "#     beta_zero=0.3,\n",
    "#     temperature=0.8,\n",
    "#     regularization=1e-2\n",
    "# )\n",
    "\n",
    "# model = RHVAE(\n",
    "#     model_config=model_config,\n",
    "#     encoder=architecture_dict['tiny']['encoder'](model_config),\n",
    "#     decoder=architecture_dict['tiny']['decoder'](model_config),\n",
    "# )\n",
    "\n",
    "model_config = VAEConfig(\n",
    "    input_dim=(1, 28, 28),\n",
    "    latent_dim=8,\n",
    ")\n",
    "\n",
    "model = VAE(\n",
    "    model_config=model_config,\n",
    "    encoder=architecture_dict['resnet']['encoder'](model_config),\n",
    "    decoder=architecture_dict['resnet']['decoder'](model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "%time count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from pythae.trainers import BaseTrainerConfig, BaseTrainer\n",
    "from pythae.pipelines.training import TrainingPipeline\n",
    "\n",
    "training_config = BaseTrainerConfig(\n",
    "    output_dir='experiments',\n",
    "    num_epochs=200,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=len(train_dataset),\n",
    "    per_device_eval_batch_size=len(eval_dataset),\n",
    "    optimizer_cls=\"AdamW\",\n",
    "    optimizer_params={\"weight_decay\": 0.05, \"betas\": (0.91, 0.99)},\n",
    "    scheduler_cls=\"ReduceLROnPlateau\",\n",
    "    scheduler_params={\"patience\": 5, \"factor\": 0.5, \"verbose\": True}\n",
    ")\n",
    "\n",
    "pipeline = TrainingPipeline(\n",
    "        training_config=training_config,\n",
    "        model=model\n",
    ")\n",
    "\n",
    "if TRAIN_FLAG:\n",
    "    pipeline(\n",
    "        train_data=train_dataset,\n",
    "        eval_data=eval_dataset,\n",
    "        #callbacks=callbacks\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythae.models import AutoModel\n",
    "\n",
    "import glob\n",
    "\n",
    "if TRAIN_FLAG:\n",
    "    trained_model = model\n",
    "else:\n",
    "    trained_model = AutoModel.load_from_folder(glob.glob('experiments/*/final_model')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECONSTRUCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = trained_model.reconstruct(eval_dataset[:25].to(device)).detach().cpu()\n",
    "\n",
    "plot_grid(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[:25].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPOLATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolations = trained_model.interpolate(eval_dataset[:5].to(device), eval_dataset[5:10].to(device), granularity=10) .detach().cpu()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=10, figsize=(10, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        axes[i][j].imshow(interpolations[i, j].cpu().squeeze(0), cmap='gray')\n",
    "        axes[i][j].axis('off')\n",
    "plt.tight_layout(pad=0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythae.samplers import NormalSampler\n",
    "\n",
    "\n",
    "sampler = NormalSampler(\n",
    "    model=trained_model,\n",
    "    sampler_config=None\n",
    ")\n",
    "gen_data = sampler.sample(\n",
    "    num_samples=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(gen_data.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythae.samplers import GaussianMixtureSampler, GaussianMixtureSamplerConfig\n",
    "\n",
    "\n",
    "gmm_sampler_config = GaussianMixtureSamplerConfig(\n",
    "    n_components=10\n",
    ")\n",
    "\n",
    "gmm_sampler = GaussianMixtureSampler(\n",
    "    sampler_config=gmm_sampler_config,\n",
    "    model=trained_model\n",
    ")\n",
    "\n",
    "gmm_sampler.fit(\n",
    "    train_data=train_dataset\n",
    ")\n",
    "\n",
    "gmm_gen_data = gmm_sampler.sample(\n",
    "   num_samples=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(gmm_gen_data.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RHVAE Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythae.samplers import RHVAESampler, RHVAESamplerConfig\n",
    "\n",
    "\n",
    "rh_sampler_config = RHVAESamplerConfig(\n",
    "    # mcmc_steps_nbr = 200,\n",
    "    # n_lf = 3,\n",
    "    # eps_lf = 1e-3,\n",
    "    # beta_zero = 0.3\n",
    ")\n",
    "\n",
    "rh_sampler = RHVAESampler(\n",
    "    sampler_config=rh_sampler_config,\n",
    "    model=trained_model\n",
    ")\n",
    "\n",
    "rh_sampler.fit(\n",
    "    train_data=train_dataset\n",
    ")\n",
    "\n",
    "rh_gen_data = rh_sampler.sample(\n",
    "   num_samples=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(rh_gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_gen_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #combined_dataset = ConcatDataset([train_dataset, rh_gen_data])\n",
    "# combined_dataset = torch.cat((train_dataset, rh_gen_data), 0)\n",
    "# combined_dataset = combined_dataset[torch.randperm(combined_dataset.size()[0])]\n",
    "# combined_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce8eda53abee1465fbe210c65d7898904902cb11262939320a138d6c4f1f5b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
